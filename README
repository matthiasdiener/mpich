

		      Portable MPI Model Implementation

			Version 1.0.11 (September 29, 1995)


This is a joint-effort project between Argonne (Bill Gropp and Rusty Lusk) and
Mississippi State (Tony Skjellum and Nathan Doss).  Hubertus Franke of IBM has
also made major contributions, as have Ed Karrels and Patrick Bridges 
(student interns at ANL).  Of course the hardest work was getting the
spec right, which was done by the MPI Forum as a whole.

The "Quick Start" section of that guide (found in mpich/doc/install.ps.Z) gives
the following steps for installation and minimal testing:

  Get mpich.tar.Z by anonymous ftp from info.mcs.anl.gov
       in the directory pub/mpi.

  (If this file is too big, you can get it in pieces from the subdirectory
   mpisplit.  If you are in Europe, look at 

    ftp://ftp.rus.uni-stuttgart.de/pub/parallelrechner/MPI/ARGONNE
    ftp://ftp.jussieu.fr/pub9/parallel/standards/mpi/anl
    ftp://ftp.ibp.fr/pub9/parallel/standards/mpi/anl
 
  )

  zcat mpich.tar.Z | tar xvf -

  cd mpich

  configure -arch=sun4 -device=ch_p4 (for example)

  make

  On workstation networks, or to run on a single workstation, edit
    mpich/util/machines/machines.sun4 file to reflect your local host
    names.  (sun4 is used here as an example; most other workstations are
    supported as well.)  On parallel machines, this step is not needed.  See
    the README file in the mpich/util/machines directory.

  cd examples/basic

  make cpi

  mpirun -np 4 cpi

  The installation and user's guides (in doc/install.ps.Z and guide.ps.Z) 
  contain helpful information on diagnosing problems; please check there 
  before sending mail to mpi-bugs (see below on how to submit bug reports).


			Changes since Version 1.0.10

1.  Fixes for the installation process (now uses the program util/mpiinstall).

2.  Access to the global clock provided on IBM SP2.

3.  mpirun now handles heterogeneous systems.

4.  A number of problems with MPI_Pack and MPI_Unpack have been fixed.

5.  Fixes for the CM-5.

6.  Some performance improvements to the ch_shmem device.

7.  A native NX version that works for all but synchronous sends is included
    as -device=nx.

8.  mpicc and mpif77 have been improved.  

9.  For workstation networks, the script 'tstmachines' provides a more 
    comprehensive test.

10. Many new comments in the installation and users guides.

11. The ch_p4 device is more robust in the presence of network failures;
    now it will terminate with an error message instead of hanging.

                               Known problems

    On Cray PVP (XMP,YMP,C90) and Cray T3D, character data is not supported
    from Fortran.  This causes  examples/test/pt2pt/structf to fail; we
    hope to have a fix in the next release.

    Don't forget to check the on-line buglist at MPICH home page
    http://www.mcs.anl.gov/mpi/mpich/index.html 

What is here
============

This directory contains files releated to the Test Implemntation of the MPI
(Message-Passing Interface) Draft Standard.  They are:

mpich.tar.Z          - the implementation itself, sufficient to run on the
                       following machines: Intel NX (i860, Delta, Paragon), 
	               IBM SP1 (using EUI or EUIH), Meiko CS-2, Ncube, 
	               IBM SP2 (using MPL), 
                       CM5 (using CMMD) and networks of workstations 
                       (Sun, SGI, RS6000, HP, DEC Alpha, ...), and symmetric
                       multiprocessors (SGI, Convex, Sun).
                       This is a link to the most recent release; you will
                       also see files of the form mpich-1.x.x.tar.Z; these
                       are the actual releases.
                       New releases are indicated by a change in the last 
                       number; the value when this readme was written
                       was 10 (as in 1.0.10).

For other systems, or to run on top of PVM 2.4 or 3.x, you also need:

chameleon-1.2.tar.Z  - the chameleon system, necessary to build and run the
                       versions of the implementation that run on top of PVM.
                       This is in pub/pdetools.  

You do not need anything else if you are using either the various MPP versions
(Intel Paragon, TMC CM5, IBM SP2) or the integrated version of p4
(-device=ch_p4; see below).

Also of interest for workstation clusters is

sut.tar.Z             - Scalable Unix Commands.  This provides versions of
                        common Unix commands, such as ls or ps, that run 
                        in parallel on a selection of machines.  This is
                        a prototype under development and is a Ptools
                        consortium project; your feedback is welcome.
                        Currently only tested on SunOS 4.1.3 and AIX 3.2.5.
                        This is NOT part of MPI.

Installing MPI
==============

Create your mpi root directory (such as ~/mpitest ).  

Ftp the files that you need to that directory.  In particular, on most 
machines you only need the one file mpich.tar.Z .

Uncompress the files (uncompress mpich.tar.Z etc) and untar them
(tar xf mpich.tar.Z etc) .  ("zcat mpich.tar.Z | tar xf - "
also works)

To create the mpi implementation, uncompress and tar the distribution, which
will give you a directory called mpich.  In this directory, you will run
configure and then make.  To see the options for configure, say

    configure -u

On many systems, you can now just say:

    configure
    make

This will try to pick an appropriate set of options.  On platforms that
do cross-compilation (e.g., CM5, Paragon, or SP2 front ends), it is necessary
to give the specific device and architecture.  These are described below.
While MPICH is building, register your copy so that you can receive
notification about future releases.  Information on registering is provided
when you run configure.

To build the network version for sun4's running SunOS 4.x, give the command

    configure -device=ch_p4 -arch=sun4
    make

To build the network version for sun4's running Solaris, give the command

    configure -device=ch_p4 -arch=solaris
    make

For SGI workstationss, use

    configure -arch=sgi -device=ch_p4
    make 

For an SGI Challenge or Power Challenge, using shared-memory for messages, use

    configure -arch=sgi -device=ch_shmem
    make

For a Convex Exemplar, use

    configure -arch=hpux -device=ch_spp
    make

For a Sun multiprocessor (Solaris), use

    configure -arch=solaris -device=ch_shmem
    make

To build for nX on an ipsc/2 or ipsc/860, use

    configure -arch=intelnx -device=ch_nx
    make

To build for nX on a paragon, use

    configure -arch=paragon -device=ch_nx
    make

To build for the Meiko CS-2, use

    configure -arch=meiko -device=meiko
    make

To build for UNICOS on a CRAY C90, CRAY Y-MP, or CRAY J90
(This will use TCP sockets for communication.)

    configure -device=ch_p4 -arch=cray
    make

To build for the Ncube, use

    configure -arch=ncube -device=ch_nc
    make

To build for the CM-5, use

    configure -arch=sun4 -device=ch_cmmd
    make
    (Not extensively tested; bug reports encouraged)

To build for SP1, SP2, or rs6000's using EUI (or MPL), give the command

    configure -device=ch_mpl -arch=rs6000 
    make

To build the Chameleon version for sun4's, using PVM 3.x, give the command

    configure -bopt=g -comm=pvm3 -arch=sun4 -device=chameleon
    make

The Makefile contains some sample targets that run configure with the
correct options for our installation; you may find these instructive.

Some useful additional flags you can give to configure include:

-mpe	Build the MPE extensions to mpi. These extensions no longer 
	require having chameleon installed, and have been tested 
	on numerous different machines.

-mpedbg Enable support for starting a debugger when an MPI error or
        signal occurs.  Currently supported only for systems that can
        start xterms running dbx.

-nof77	Don't build the fortran interface. This option has only been
	lightly tested, but should work.

-prefix=<location>
	Use <location> as the top-level directory to install MPI in
	when installing. This option can also be set at install time
	by specifiying PREFIX=<location> to the make install. 

During the configure, you may need to supply the locations of the Chameleon
(tools.core) installation and optionally the p4 or PVM installation.  These
locations should be supplied as full path names.

To use mpirun on a homogeneous network, make the file
util/machines/machines.<arch> to be a list of machines that mpirun can use to
run jobs. Example machine files are installed in util/machines.anl.

After building a specific version as above, you can install it with

    make install		  

if you used -prefix at configure time, or,

    make install PREFIX=/usr/local/mpi    (or whatever directory you like)

if you did not, or want to override the configure-time selection.
If you intend to leave MPI where you built it, you should NOT install it 
(install is used only to move the necessary parts of a built MPICH to another
location).

The installed copy will have the include files, libraries, man pages, and a
few other odds and ends, but not the whole source tree and examples.  There is
a small examples directory for testing the installation and a
location-independent Makefile built during installation, which users can copy
and modify to compile and link against the installed copy.

To rebuild a different version, say 

    make clean

to clean up everything, including the Makefiles constructed by configure,
and then run configure again with the new options.

Testing MPI
===========
The directory "examples/test" contains makefiles and scripts for testing 
the implementation.  If you have set up mpirun (see "Running Programs" below)
correctly, then

cd examples/test
make testing

will run a variety of test programs.  

If you have not setup mpirun, you can still use the test programs in these
directories; you'll just have to write your own script to run them.  
(And send mail to mpi-bugs describing what problems you had with mpirun.)

Running Programs
================

How you start an MPI program depends on the type of device (the -device 
option to configure).  You can also use mpirun to start jobs on many
machines. Here are the choices:

*** mpirun *** 

"mpirun" is a shell script that attempts to hide the differences in
starting jobs for various devices from the user. Mpirun attempts to
determine what kind of machine it is running on and start the required
number of jobs on that machine. On workstation clusters, if you are
not using chameleon, you must supply a file that lists the different
machines that mpirun can use to run remote jobs or specify this file
every time you run mpirun with the -mr_machine file option. The default
file is in util/machines/machines.<arch>.


mpirun typically works like this:
mpirun -mr_np <number of processes> <program name and arguments>

If mpirun can't determine what kind of machine you are on, and it 
is supported by the mpi implementation, you can the -mr_machine
and -mr_arch options to tell it what kind of machine you are running
on. The current valid values for mr_machine include:

              chameleon (including chameleon/pvm, chameleon/p4, etc...)
              meiko     (the ch_meiko device on the meiko)
              paragon   (the ch_nx device on a paragon not running NQS)
              p4        (the ch_p4 device on a workstation cluster)
              sp1       (ch_eui on ANL's sp2)
	      execer    (a custom script for starting ch_p4 programs
			 without using a procgroup file. This script
                         currently does not work well with interactive
			 jobs)

You should only have to specify mr_arch if mpirun does not recognize
your machine, the default value is wrong, and you are using the p4 or
execer devices. Other options include:

    -h   This help
    -machine <machine name>
         use startup procedure for <machine name>
    -machinefile <machine-file name>
	 Take the list of possible machines to run on from the"
         file <machine-file name>"
    -np <np>
         specify the number of processors to run on
    -nolocal
         don't run on the local machine (only works for 
    -e   Use execer to start p4 programs on workstation
         clusters
    -pg  Use a procgroup file to start p4 programs, not execer
	 (default)
    -leave_pg
         Don't delete the P4 procgroup file after running
    -t   Testing - do not actually run, just print what would be
         executed
    -v   Verbose - thrown in some comments

If you don't want to use mpirun, use the instructions below depending
on which device you have configured your program:
 
*** device=ch_p4 ***
The processors are specified by a p4 procgroup file

Procgroup files contain information about which machines a network of
processes should run on.  

A typical procgroup file looks like:

local 0
sun1.foo.edu 1 /home/me/myprog
sun2.foo.edu 1 /home/me/myprog
sun3.foo.edu 1 /home/me/myprog

to run with 4 processes, one on the machine where the program is executed,
(aloways called "local", and the others on the machines sun1, sun2, and sun3.
The "local 0" means no second process on the local machine.

To run, say on sun0, call the above file something like myprog.pg and do

  myprog -p4pg myprog.pg

More elaborate and flexible procgroup files are possible.  See the p4 manual
for details.  Running on parallel machines often requires a procgroup file
containing "local 15" (for 16 = 15 - 1 processes).  The job also will need
to be started with the appropriate method for a parallel job on that system.

*** device=ch_nx: ***
Just start the job like any Intel NX program

*** device=ch_eui ***
Just start the job like any EUI-H or EUI program.  For example, in EUI-H

/usr/lpp/euih/eui/cotb0 -b <programname> <number_of_nodes> <commandline args>

*** device=chameleon ***
Use the commandline arguments "-np <number_of_nodes>" with the program.
For example, to run the program "first" with two processes, 

    first -np 2

In Chameleon, you can add the option "-trace" to get a trace of the
communication operations. The option "-chdebug" gives more information on
message contents as well. 
p
Follow the Chameleon installation instructions for specifying the available
processes when using PVM or p4.

Installing MPI for the Chameleon Device
=======================================

If you are using p4 with Chameleon, first make p4 with

    cd p4-1.4
    make P4ARCH=SUN		(for example)
    cd ..
    
(You should NOT use the the p4 in mpid/ch_p4/p4-1.4; instead, get p4 
from the distribution directory (pub/mpi)).

To make chameleon, 

    setenv P4DIR <the root directory for p4>
    cd tools.core                            (the home of Chameleon)
    bin/inscheck sun4 -libs g                (test that install will succeed)
    bin/install sun4 -libs g >& make.log     (for example)
    <Create a "hosts" or "pvmhosts"file>     (for P4 or PVM)
      to create hosts file for P4...
	cd comm
	cp hosts.sample hosts
        <Edit hosts>
        cd ..
    cd ..

If you encounter any difficulties that seem to point to an incomplete 
Chameleon installation, please send us the 'make.log' file.
(see the readme in the top level Chameleon directory for more information, 
particularly for the "hosts" file).  There is a sample hosts file in
tools.core/comm/hosts.sample.

If you need to clean and re-install Chameleon, use "make cleanall".

(Note added in proof: Chameleon may have a configure-based installation
by the time you read this.  Check the readme file in the top-level Chameleon
directory.)

Details of this Implementation 
==============================

The interesting stuff is in include/mpir.h and mpid/chameleon/mpid.h .  The
implementation of the device-independent part is in src ; the device
implementation (to the abstract device) in in mpid (currently, only the
Chameleon "device" is implemented; a version of the device built directly on
nx by Paul Pierce of intel is included, but is not quite finished).  Chameleon
supports (directly) Intel NX, IBM EUI and EUI-H, TMC CMMD (Version 2 and
greater), as well as p4, PVM, and PICL.

Some notes on using Chameleon
=============================

There is a "readme" in ./chameleon that you should look at.
For networks, chameleon uses a "hosts" file (in ./chameleon/comm/hosts )
that contains the names of available machines as well as some limits on their
use.  The file ./chameleon/comm/hosts.sample contains a sample hosts file.

On parallel machines (like the i860 and CM-5), the hosts file is ignored.
Instead, you must get your program loaded on the parallel machine.
For example, to build and run the C examples for the i860, do:

    cd mpi/examples_c
    make BOPT=g ARCH=intelnx
    getcube -t d1
    load ring -np 2
    waitcube
    load twin -np 2
    waitcube
    relcube

(you will have to build chameleon and mpi for the intelnx; this is necessary
only if you use Chameleon instead of the native Intel NX port.  Note that you
will still need to startup your program in whatever way is correct for your
system; Intel Paragon users may need to use "pexec"; Intel Delta users need to
use "mexec".)

The PVM version is built much in the same way as the P4 version; just 
do

    setenv PVMDIR <root directory of PVM>

before building chameleon.  Use "COMM=pvm" instead of "COMM=p4".  For PVM 3.x,
use "COMM=pvm3"

Please send problem reports to mpi-bugs@mcs.anl.gov.  Please include:

     The version of MPICH (e.g., 1.0.10)

     The output of running your program with the -mpiversion argument
     (e.g., mpirun -np 1 a.out -mpiversion)

     The output of 
	uname -a
     for your system.  If you are on an SGI system, also
	hinv

     If the problem is with a script like configure or mpirun, run the
     script with the -echo argument 
     (e.g., mpirun -echo -np 4 a.out )

     If you are using a network of workstations, also send the output of
     bin/tstmachines or util/tstmachines.

     




	  Information from previous README's, of historical interest


			Changes since Version 1.0.10

1.  Fixes for the installation process (now uses the program util/mpiinstall).

2.  Access to the global clock provided on IBM SP2.

3.  mpirun now handles heterogeneous systems.

4.  A number of problems with MPI_Pack and MPI_Unpack have been fixed.

5.  Fixes for the CM-5.

6.  Some performance improvements to the ch_shmem device.

7.  A native NX version that works for all but synchronous sends is included
    as -device=nx.

8.  mpicc and mpif77 have been improved.  

9.  For workstation networks, the script 'tstmachines' provides a more 
    comprehensive test.

10. Many new comments in the installation and users guides.

11. The ch_p4 device is more robust in the presence of network failures;
    now it will terminate with an error message instead of hanging.

                               Known problems

    On Cray PVP (XMP,YMP,C90) and Cray T3D, character data is not supported
    from Fortran.  This causes  examples/test/pt2pt/structf to fail; we
    hope to have a fix in the next release.

For other systems, or to run on top of PVM 2.4 or 3.x, you also need:

chameleon-1.2.tar.Z  - the chameleon system, necessary to build and run the
                       versions of the implementation that run on top of PVM.
                       This is in pub/pdetools.  




			Changes since Version 1.0.9 

1.  Some fixes for Cray T3D (-device=t3d) for MPI_Test and friends

2.  Fix for CM-5 CMMD (but untested as we don't have access to a CM-5)

3.  Improvements in thread locking for device writers (see
    mpid/chameleon/dmch.h)  (More work is still needed)

4.  Reorganized include files (internal only; user MPI programs are unchanged)
    thanks to SGI.

5.  Internal reorganization of collective code to allow easier optimization
    by vendors (thanks to Meiko)

6.  New version of mpirun (using sh instead of csh) is easier to add devices 
    to; also has limited ablitity to manage heterogeneous runs.

7.  New commands mpicc and mpif77 make it easier to run simple programs.
    They also provide simplified interfaces to the MPE profiling libraries

8.  Now runs on AIX SMPs.
                         Changes since Version 1.0.8

1.  Improved support for heterogeneous systems with different data lengths
    (more remains to be done).  Unified much of the code to pack and 
    unpack datatypes.  Note that MPICH has used a "sender converts" 
    approach since 1994 in preference to XDR (which is still used in
    some cases).  Extensions to this mechanism will further reduce the
    usage of XDR in heterogeneous systems.

2.  Fixes to mpireconfig and upshot when installing a distribution.

3.  Added new profiling libraries for tracing, log file generation, and
    realtime animation.  See the Users Guide.

4.  Shortened some filenames to 12 characters (plus extension); this helps
    Cray YMPs that corrupt their object libraries when presented with an
    otherwise valid filename with more than 12 characters.

5.  Added a native Meiko CS-2 device (thanks to Jim Cownie).

6.  Updated (in part) to conform to clarifications of the MPI Forum.  This
    includes:

    Changed behavior when empty lists of requests are given to TESTxxx/WAITxxx

    Many MPI pre-defined datatypes are now compile-time constants (all will
    be soon).

    Some man pages have been updated to clarify the definition.  See 
    in particular MPI_Cart_shift and MPI_Bsend.

    A new constant, MPI_BUFFER_OVERHEAD, can be used with MPI_BUFFER_ATTACH.

7.  Some bugs in the code for the CM5 MAY have been fixed; we no longer 
    have access to any CM5 so we can't test it (NPAC CM5 is no longer 
    accessible, and accounts on the Los Alamos CM5 are limited to 
    Los Alamos grand challenge projects and industrial partners).  

8.  Bugs in the Intel Paragaon version have been fixed, particularly those
    in the handling of nonblocking operations.
    
9.  Many additions to the test suite.

March, 1995 (1.0.8)
New in the 1.0.8 release are separate Users and Installation guides.  

1.  The ch_p4 device for HPs now handles large messages more reliably 
    (HPUX, alone of Unix varients, uses EAGAIN instead of EWOULDBLOCK
    for the write-would-block condition).

2.  The script util/mpireconfig makes it easier to build Makefiles to fit
    a configuration

3.  The script util/tstmachines helps identify problems in using workstation
    clusters.

4.  The code for MPI_Reduce (and MPI_Allreduce) now handles non-commutative
    operators correctly.  Previous versions swapped the first two arguments
    to the reduction function.  See examples/test/coll/coll10.c for an
    example non-commutative operation.

5.  A bug in the handling of large numbers of sends without any receives has
    been fixed.

6.  A new device, ch_shmem, provides a shared-memory implementation for SGI
    Challenge, Power Challenge, and Onyx, Convex Exemplar, and Sun
    multiprocessors running Solaris.  Special features include the ability to
    handle very large messages (by transfering them in pieces).  See below
    for how to configure for these machines.

7.  Nonblocking operations involving MPI_PROC_NULL now work correctly

8.  A new Users Guide and a separate Installation guide are now distributed
    in the 'doc' directory

9.  The 'secure server' for faster startup on workstation clusters is now
    available and its use documented in the Users and Installation guides.

10. Fixed failure in MPI_Cart_coords on the second call.

11. Many changes to configure, including code to find wish, tcl, and tk for
    upshot and choose xlC over xlc on rs6000's.

12. Now dynamically determines the size of Fortran datatypes for systems
    where sizeof(float) != sizeof(Fortran real)

13. Tests in examples/test are now deterministic (no longer assumes a 
    particular output order).

14. New MPE_Fill_circle routine (and mpe/contrib/mastermind example that
    uses it)

15. Global operations (reduce, scan) now handle long double type.

16. Misc. bugs in the code and the test programs have been fixed.

17. The ch_p4 device can now be used with -arch=symmetry and -arch=symm_ptx
    for Sequent Symmetry shared-memory multiprocessors.


Jan 16, 1995 (1.0.7)
1.  Configure now checks for C compilers that can't properly evaluate
    (a) && (b).  At least our IBM RS6000 xlc compiler can't get this correct,
    and the internal implementation depends on it.  Unfortunately, as a 
    result, possibly many sites can no longer build
    MPICH for the IBM SP1/SP2.  Of course, with the C compiler generating 
    incorrect code, why you'd want to run on them is an interesting question.

    A second test has been added; IBM compilers with the fix for the first
    may fail with the second.  

2.  Many small fixes to memory usage and the MPI datatype reference counts 
    (used to know when to recover the space associated with a datatype).

3.  There are routines to print out the action of an MPI datatype in
    mpe/examine.c .  These are preliminary; comments are welcome.

4.  In mpirun, solaris is now a different architecture from sun4.  Make
    sure that you have a machines.solaris file (previously, you needed
    a machines.sun4 file).

Dec 23, 1994
1.  -p4pg can be supplied to mpirun to override mpirun's construction of 
    a p4 procgroup file.

2.  See http://www.mcs.anl.gov/home/lusk/mpich/index.html for current status
    and additional devices.

3.  A preliminary Parix implementation for Parsytec-PowerPC based machines 
    (Parix 1.3) has been done by another group.  Contact Lutz Laemmer 
    (laemmer@iib.bauwesen.th-darmstadt.de) for more information.  Currently,
    this implementation is NOT included with MPICH.

Dec 10
1.  The -mpedbg and -memdebug options have been added to configure; 
    these can aid in debugging MPI and MPI applications.  -mpedbg works
    on systems that support xterm and dbx; this will pop up an xterm
    running dbx when the program gets either a common fatal signal (e.g.,
    SIGSEGV) or an MPI error.  -memdebug adds extensive memory debugging,
    including tests for overwrites and freeing the same data twice.

2.  MPI protects the user-visible data structures (such as MPI_COMM and
    MPI_ERRHANDLER) more aggressively.  Passing a corrupted, incorrect, or
    previously freed structure will now (where feasible) generate an error
    message.

3.  Additional changes to support 64-bit systems have been made.

4.  MPI_BOTTOM may now be used in Fortran programs.


Oct 26
1.  The MPI ADI (low level interface that performs the actual message-passing)
    has been reorganized and given additional flexibility.  See 
    configure -usage for a short summary of available options.  At this time,
    they are still undergoing testing.

2.  The Meiko CS2 has been added as a "device".  This version uses the NX
    compatibility library.  Only partially tested.

See the end of the file for a history of changes.  These document a number of
features that are currently not described elsewhere (we ARE working on a 
user's guide...)
   
The implementation gets its portability without giving up efficiency by being
implemented in terms of an Abstract Device Interface (ADI).  The ADI itself is
described in the paper mpiadi.ps.Z in the doc directory. (This is still a
draft.)  The ADI is implemented on various parallel machines and workstation
networks. It can be configured to use either p4, PVM 2.4, or PVM 3.x on
networks.  Note that the ADI is NOT layered on top of only p4 or PVM; through
macro processing, it sits directly on any of a wide range of vendor
message-passing or shared-memory systems.  

The purpose of this initial implementation is to advance the cause of MPI by:

1.  demonstrating the implementability of MPI and contributing to shaking
    down the specifications.

2.  providing an avenue for applications to begin the porting process very
    early.

3.  providing a shortcut path for vendor implementations.  Vendors need
    provide only the device part of the implementation.

Longer-term goals are to provide a portable MPI implementation for
heterogeneous networks of workstations and to motivate research into device
abstractions for high-performance message-passing.

The current implementation has the following limitations, as of 
October 20, 1994.

1.  Much of the code written so far is written for clarity and simplicity
    rather than efficiency.  However, some functions have been tuned to
    provide examples to vendors of how easy it is to improve the
    performance.  Some performance results are presented in doc/perf.ps.Z

2.  The code is not thoroughly or systematically tested.

3.  There is a further list of things that need immediate attention in the
    'todo' file in the top directory.

4.  Heterogeneity is now partially supported.  Systems that differ only in
    the ordering of the bytes (big endian versus little endian) are 
    supported EXCEPT FOR MPI_PACK/MPI_UNPACK.  Full support using XDR is
    under development.

On the other hand, here are a few good aspects of the current state:

1.  The Chameleon-p4 (and Chameleon-PVM) implementation is portable to
    virtually any parallel machine and to heterogeneous networks of
    workstations.  The implementation is not yet highly optimized but it is
    not naive.  Most of the testing has been on networks of Suns and RS/6000's.

2.  A major application (a nuclear-structure code from Argonne's Physics
    Division) has been ported to MPI and runs on the IBM SP1.  Other
    applications have been ported to the IBM version of MPI at IBM.


----------------------------------------------------------------------------
Previous "new features"
----------------------------------------------------------------------------

Sep 30
1.  MPI Data structures now contain a token that is used to recognized an
    invalid or incorrect object (for example, an uninitialized MPI_Request
    given to MPI_Wait).  These won't catch everything, but they may help 
    with such things as out-of-date mpif.h files used in Fortran programs,
    and modules that need to be recompiled.

2.  Fortran programs that use MPICH must be recompiled (the common blocks in
    mpif.h have changed)

3.  MPICH is now (nearly) 64 bit safe, at least from C.  

4.  Configure works harder to identify which of many mutually incompatible
    configurations are being built when running on SGIs.  A native shared
    memory version of -device=ch_p4 is available by selecting -comm=shared.
    This version works but needs additional work to get better performance.

5.  The internal structure and the interface of the ADI has changed.  We hope
    to have support for multiple protocols (not just eager) and multiple ADIs,
    as well as support for systems with remote memory operations, in the next
    major release.

Sep 14
1.  Our goal is a full MPI implementation, and it is complete.  This
    release now includes both Fortran and C interfaces, for all of
    point-to-point, collective, groups and contexts, topologies, and the
    profiling interface.

2.  The implementation is portable to all parallel machines and workstations
    where either Chameleon, p4, or PVM run.  We have done most of our own
    testing on Suns, RS6000's, and the SP1.  Minimal checkout has been done
    on the Intel IPSC and Paragon, CM5, Ncube, IBM SP1, and on SGI's amd
    FreeBSD.  Note that for MPPs, you do not need to install Chameleon, p4, 
    or PVM; you can use the "native devices" for those systems.

3.  We have introduced a version number (1.3, in include/patchlevel.h).  
    The command line argument, -mpiversion, will display the current 
    version.  Please send us the version number when reporting bugs.

4.  A number of new devices have been implemented; when using these 
    devices, it is not necessary to install Chameleon.  These include
        ch_eui   - IBM EUI (for SPx) 
        ch_nc    - nCUBE                
        ch_cmmd  - CM5
        ch_nx    - Intel NX
        ch_p4    - p4
    The `ch_p4' device uses a version of p4 that has been integrated with
    MPI; when using this version, only the mpi library (and any relevent
    system libraries, such as -lbsd or -lsocket) needs to be linked.
    There is no need to separately obtain and install p4 (or Chameleon).
    See below on using the ch_p4 device.

5.  We have done some tuning since the last release, and the point-to-point
    routines are fast.  There are graphs summarizing some of our comparisons
    with other systems in the doc directory, in perf.ps.Z .  If
    you find that this implementation of MPI is not at least 95% as fast as
    what you are currently using on any machine or network, please let us know
    and we will try to fix it.  (We are not yet at top speed on the Paragon,
    due to having to layer on top of the Intel protocol, but are working on
    it.)  The option -nodevdebug to configure removes rather extensive 
    debugging code from the implementation and should be considered once
    you are sure that the installation has been successful.  Other tuning
    parameters are described with 'configure -usage'.

6.  There is a library of add-ons we call MPE (for Multi-Processing
    Environment) that is a first step toward getting some helpful parts of a
    programming environment to go with MPI.  It includes a logging package and
    a parallel simple graphics library (parallel in the sense that multiple
    processes share an X display).  This stuff is found in the mpe
    subdirectory.  Some of the examples demonstrate its use.

7.  There is a Tcl/Tk version of upshot in profiling/upshot, for viewing
    the log files produced with the MPE logging routines.  This is in 
    profiling/upshot/bin ; upshot can now read (old) PICL trace files as well.
    Sample tracefiles, both produced by the MPI/MPE profiling interface and
    PICL (from the ParaGraph distribution) are in profiling/upshot/logfiles .
    Some of these are compressed (with the Unix "compress" program); you 
    will need to uncompress them before giving them to upshot.

8.  There are many more examples, in the examples directory.  We have 
    undertaken a reorganization of the examples, including the creating of
    a test suite (in the directory examples/test).  This effort is still 
    underway.  We are now putting programs that illustrate the use of MPI
    into examples/contrib.  We solicit contributions.

9.  There are man pages for all the MPI routines.  Put .../mpich/man in your
    MANPATH (or see the script mpich/util/mpiman).

10. There are some dubugging aids.  The command-line option -mpiqueue will
    dump the receive queues at the time of MPI_Finalize, so you can see where
    lost messages went.  The -chdebug option on the command line will enable
    Chameleon tracing of events, which may be helpful (but only when using
    the Chameleon device).

New in the August 8, 1994 version:
  
  Various bug fixes 
      Attributes in Fortran
      MPI_Waitall
      
  Support for heterogenity between some systems
      We support the transmission of contiguous data between processors that
      have the same basic representations but use different byte orderings.
      We encourage you to try these out and let us know of any problems.
      The MPI_PACKED datatype, and MPI_PACK and MPI_UNPACK are not yet
      supported for heterogeneous systems.  

  Improved configuration (now detects strange Fortran external names, 64 bit
  pointers, and the like).

  Support for the DEC Alpha and other 64-bit systems

  "New in the <previous release>" has been condensed in the README

New in the July 22, 1994 version:

  Native NCube and CMMD devices now exist and appear to work correctly
  (-device=ch_nc and -device=ch_cmmd, respectively).  MPIRUN does not
  yet run on these machines. Start MPI jobs on these machines like you
  would start any other parallel job.

  Compiling with MPE does not require having Chameleon or PETSc installed

  MPIRUN has been greatly enhanced

  Pathnames in installed shell scripts have been fixed

New in the July 13, 1994 version:

  Tested on the Meiko CS2 (configure -device=ch_p4 -arch=meiko)
  (NOTE THAT -device=ch_meiko now 10/26/94)

  Tested on the SGI Onyx

  Enhanced upshot that can read ParaGraph log files

  Better installation procedures (make install PREFIX=/usr/local/mpi).  The
    examples subdirectory will contain a location-independent Makefile
    that can be copied and modified for your programs.

  Better support for sites without Fortran  (configure .... -nof77)

  MPI executables built with the ch_p4 device can be started by the next
    release of DQS, which is currently being tested.

New in the late May, 1994 version:

  MPI_NULL_FN does not exist.  Instead, you should use MPI_NULL_COPY_FN
  or MPI_NULL_DELETE_FN as appropriate when creating keyval's using 
  MPI_KEYVAL_CREATE.

New in the April 12, 1994 version:

  The switch "-nof77" to configure allows you to build a C-language only
  version of MPI (for those of you without Fortran compilers). 

  The toplevel Makefile contains some example targets for running 
  configure and then building the MPI libraries.  One target, "anlall", 
  is used at ANL to build the test versions.

  The libraries are now in lib/$(ARCH)/$(COMM); this allows you to build
  multiple transport layers for a given system (e.g., sun4 p4 and pvm3).
  A word of warning; while we have run an MPI program with both PVM 2.4.2
  and PVM 3.x, the PVM interfaces have not been extensively tested (we use
  P4 for workstations and the native vendor message-passing for MPPs).

  The "worker" interface has been eliminated; programs must start
  with main.

  An MPI "Reference card" is in mpich/util/functions.{dvi,ps} .

  ANSI C prototypes are provided by default (included by mpi.h) for 
  ANSI C compilers.

New in the Feb 7, 1994 version:

  The MPE (MultiProcessing Environment "helper" library has been begun, and
  contains some simple timing routines.  Soon there will be logging routines.

New in the Dec 3, 1993 version:

  The installation has been completely changed, to use gnu autoconf.  (You
  don't need autoconf;  rather, you will run the enclosed configure script,
  which was generated with autoconf.)
